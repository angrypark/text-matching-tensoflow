{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름만 바꿔서 실행하면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from dataset import Dataset\n",
    "from trainer import MatchingModelTrainer\n",
    "from preprocessor import Preprocessor\n",
    "from utils.dirs import create_dirs\n",
    "from utils.logger import SummaryWriter\n",
    "from utils.config import load_config, save_config\n",
    "from models.base import get_model\n",
    "from utils.utils import JamoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"delstmdense4_512_batch1024_nsrandom4_adam_lrt2t_bugfixed\"\n",
    "TOKENIZER = \"SentencePieceTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/{}/\".format(NAME)\n",
    "config_dir = base_dir + \"config.json\"\n",
    "best_model_dir = base_dir + \"best_loss/best_loss.ckpt\"\n",
    "# best_model_dir = base_dir + \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = load_config(config_dir)\n",
    "preprocessor = Preprocessor(model_config)\n",
    "\n",
    "infer_config = load_config(config_dir)\n",
    "setattr(infer_config, \"tokenizer\", TOKENIZER)\n",
    "setattr(infer_config, \"normalizer\", \"MatchingModelNormalizer\")\n",
    "setattr(infer_config, \"soynlp_scores\", \"/media/scatter/scatterdisk/tokenizer/soynlp_scores.sol.100M.txt\")\n",
    "infer_preprocessor = Preprocessor(infer_config)\n",
    "infer_preprocessor.build_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Base learning rate: 2.000000\n",
      "No pre-trained embedding found, initialize with random distribution\n",
      "INFO:tensorflow:Trainable Variables Total size: 25010690\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      ":::MLPv0.5.0 transformer 1543213855.970318556 (/home/angrypark/envs/angryenv/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:45) opt_name: \"Adam\"\n",
      ":::MLPv0.5.0 transformer 1543213855.980309963 (/home/angrypark/envs/angryenv/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_beta1: 0.9\n",
      ":::MLPv0.5.0 transformer 1543213855.989375830 (/home/angrypark/envs/angryenv/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_beta2: 0.997\n",
      ":::MLPv0.5.0 transformer 1543213855.998519897 (/home/angrypark/envs/angryenv/lib/python3.6/site-packages/tensor2tensor/utils/optimize.py:45) opt_hp_Adam_epsilon: 1e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angrypark/envs/angryenv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/scatter/scatterdisk/reply_matching_model/runs/delstmdense4_512_batch1024_nsrandom4_adam_lrt2t_bugfixed/best_loss/best_loss.ckpt\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "with graph.as_default():\n",
    "    Model = get_model(model_config.model)\n",
    "    data = Dataset(preprocessor, \n",
    "               model_config.train_dir, \n",
    "               model_config.val_dir, \n",
    "               model_config.min_length, \n",
    "               model_config.max_length,\n",
    "               model_config.num_negative_samples,\n",
    "               model_config.batch_size, \n",
    "               model_config.shuffle, \n",
    "               model_config.num_epochs, \n",
    "               debug=False)\n",
    "    infer_model = Model(data, model_config, mode=tf.contrib.learn.ModeKeys.EVAL)\n",
    "    infer_sess = tf.Session(config=tf_config, graph=graph)\n",
    "    infer_sess.run(tf.global_variables_initializer())\n",
    "    infer_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "infer_model.load(infer_sess, model_dir=best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(preprocessor):\n",
    "    base_dir = \"/home/angrypark/reply_matching_model/data/\"\n",
    "    with open(os.path.join(base_dir, \"test_queries.txt\"), \"r\") as f:\n",
    "        test_queries = [line.strip() for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_replies.txt\"), \"r\") as f:\n",
    "        replies_set = [line.strip().split(\"\\t\") for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_labels.txt\"), \"r\") as f:\n",
    "        test_labels = [[int(y) for y in line.strip().split(\"\\t\")] for line in f]\n",
    "\n",
    "    test_queries, test_queries_lengths = zip(*[preprocessor.preprocess(query)\n",
    "                                                     for query in test_queries])\n",
    "    test_replies = list()\n",
    "    test_replies_lengths = list()\n",
    "    for replies in replies_set:\n",
    "        r, l = zip(*[preprocessor.preprocess(reply) for reply in replies])\n",
    "        test_replies.append(r)\n",
    "        test_replies_lengths.append(l)\n",
    "    return test_queries, test_replies, test_queries_lengths, test_replies_lengths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sess, preprocessor):\n",
    "    test_queries, test_replies, test_queries_lengths, \\\n",
    "    test_replies_lengths, test_labels = load_test_data(preprocessor)\n",
    "\n",
    "    # flatten\n",
    "    row, col, _ = np.shape(test_replies)\n",
    "    test_queries_expanded = [[q]*col for q in test_queries]\n",
    "    test_queries_expanded = [y for x in test_queries_expanded for y in x]\n",
    "    test_queries_lengths_expanded = [[l]*col for l in test_queries_lengths]\n",
    "    test_queries_lengths_expanded = [y for x in test_queries_lengths_expanded for y in x]\n",
    "    test_replies = [y for x in test_replies for y in x]\n",
    "    test_replies_lengths = [y for x in test_replies_lengths for y in x]\n",
    "\n",
    "    feed_dict = {model.input_queries: test_queries_expanded,\n",
    "                 model.input_replies: test_replies,\n",
    "                 model.query_lengths: test_queries_lengths_expanded,\n",
    "                 model.reply_lengths: test_replies_lengths, \n",
    "                 model.embed_dropout_keep_prob: 1, \n",
    "                 model.lstm_dropout_keep_prob: 1, \n",
    "                 model.dense_dropout_keep_prob: 1}\n",
    "    probs = sess.run(model.positive_probs, feed_dict=feed_dict)\n",
    "    probs = np.reshape(probs, [row, col])\n",
    "    return test_labels, probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(model, sess, preprocessor):\n",
    "    test_queries, test_replies, test_queries_lengths, \\\n",
    "    test_replies_lengths, test_labels = load_test_data(preprocessor)\n",
    "\n",
    "    # flatten\n",
    "    row, col, _ = np.shape(test_replies)\n",
    "    test_queries_expanded = [[q]*col for q in test_queries]\n",
    "    test_queries_expanded = [y for x in test_queries_expanded for y in x]\n",
    "    test_queries_lengths_expanded = [[l]*col for l in test_queries_lengths]\n",
    "    test_queries_lengths_expanded = [y for x in test_queries_lengths_expanded for y in x]\n",
    "    test_replies = [y for x in test_replies for y in x]\n",
    "    test_replies_lengths = [y for x in test_replies_lengths for y in x]\n",
    "\n",
    "    feed_dict = {model.input_queries: test_queries_expanded,\n",
    "                 model.input_replies: test_replies,\n",
    "                 model.query_lengths: test_queries_lengths_expanded,\n",
    "                 model.reply_lengths: test_replies_lengths, \n",
    "                 model.embed_dropout_keep_prob: 1, \n",
    "                 model.lstm_dropout_keep_prob: 1,\n",
    "                 model.num_negative_samples: 4, \n",
    "                 model.dense_dropout_keep_prob: 1}\n",
    "    probs = sess.run(model.probs, feed_dict=feed_dict)\n",
    "    probs = [p[1] for p in probs[:len(test_replies)]]\n",
    "    probs = np.reshape(probs, [row, col])\n",
    "    return test_labels, probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_replies(model, sess, preprocessor, queries, replies=None, topk=10):\n",
    "    if not replies:\n",
    "        with open(\"../reply_matching_model/data/reply_set_new.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            reply_set = [line.strip() for line in f]\n",
    "        indexed_reply_set, reply_lengths = zip(*[preprocessor.preprocess(reply) for reply in reply_set])\n",
    "        indexed_queries, query_lengths = zip(*[preprocessor.preprocess(query) for query in queries])\n",
    "        reply_size = len(reply_set)\n",
    "    else:\n",
    "        reply_set = replies\n",
    "        indexed_reply_set, reply_lengths = zip(*[preprocessor.preprocess(reply) for reply in replies])\n",
    "        indexed_queries, query_lengths = zip(*[preprocessor.preprocess(query) for query in queries])\n",
    "        reply_size = len(replies)\n",
    "    \n",
    "    best_replies = list()\n",
    "    for query, indexed_query, length in tqdm(zip(queries, indexed_queries, query_lengths)):\n",
    "        feed_dict = {model.input_queries: [indexed_query]*reply_size,\n",
    "                     model.input_replies: indexed_reply_set,\n",
    "                     model.query_lengths: [length]*reply_size,\n",
    "                     model.reply_lengths: reply_lengths, \n",
    "                     model.embed_dropout_keep_prob: 1, \n",
    "                     model.lstm_dropout_keep_prob: 1, \n",
    "                     model.num_negative_samples: 1}\n",
    "        probs = sess.run(model.probs, feed_dict=feed_dict)\n",
    "        probs = [p[1] for p in probs]    \n",
    "        result = sorted([[reply, score] for reply, score in zip(reply_set, probs)], key=lambda x: x[1], reverse=True)[:topk]\n",
    "        best_replies.append([query, result])\n",
    "    return best_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../reply_matching_model/data/reply_set_new.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    replies = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_replies = custom_replies + replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_replies = list(set(custom_replies))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hackathon(model, sess, preprocessor):\n",
    "    with open(\"../paraphrase_detection/data/test_queries.txt\", \"r\") as f:\n",
    "        test_queries = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    with open(\"../paraphrase_detection/data/test_replies.txt\", \"r\") as f:\n",
    "        test_replies = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    \n",
    "    preprocessed_replies, replies_lengths = zip(*[preprocessor.preprocess(sentence) for sentence in test_replies])\n",
    "    length = len(preprocessed_replies)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        preprocessed_query, _ = preprocessor.preprocess(query)\n",
    "        feed_dict = {model.input_queries: [preprocessed_query]*length,\n",
    "                 model.input_replies: preprocessed_replies,\n",
    "                 model.queries_lengths: [len(query)]*length,\n",
    "                 model.replies_lengths: replies_lengths,\n",
    "                 model.dropout_keep_prob: 1}\n",
    "        probs = model.infer(sess, feed_dict=feed_dict)\n",
    "        probs = [(i, prob) for i, prob in enumerate(probs)]\n",
    "        probs = [(i, reply, prob) for reply, (i, prob) in zip(test_replies, probs)]\n",
    "        select = [line[1] for line in sorted(probs, key=lambda x: x[2], reverse=True)[:3]]\n",
    "        print(i, query, select)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result = get_best_replies(infer_model, infer_sess, infer_preprocessor, queries, custom_replies, topk=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for r in result:\n",
    "    print(r[1][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_prob, k=5):\n",
    "    def get_rank(y_true, y_prob):\n",
    "        rs = list()\n",
    "        for y_t, y_p in zip(y_true, y_prob):\n",
    "            r = sorted([(t, p) for t, p in zip(y_t, y_p)], key=lambda x: x[1], reverse=True)\n",
    "            r = [t for t, p in r]\n",
    "            rs.append(r)\n",
    "        return rs\n",
    "\n",
    "    def get_precision_at_k(rs, k):\n",
    "        rs = [(np.asarray(r)[:k] != 0) for r in rs]\n",
    "        return np.mean([np.mean(r) for r in rs])\n",
    "    \n",
    "    def mean_reciprocal_rank(rs):\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    def dcg_at_k(r, k):\n",
    "        r = np.asfarray(r)[:k]\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    \n",
    "    def ndcg_at_k(r, k):\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k) / dcg_max\n",
    "    \n",
    "    def mean_ndcg_at_k(rs, k):\n",
    "        return np.mean([ndcg_at_k(r, k) for r in rs])\n",
    "    \n",
    "    def flatten(list_of_lists):\n",
    "        return [y for x in list_of_lists for y in x]\n",
    "    \n",
    "    def get_best_threshold(y_true, y_prob):\n",
    "        y_true_binary = [y!=0 for y in flatten(y_true)]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true_binary, flatten(y_prob))\n",
    "        best_f_measure = 0\n",
    "        best_threshold = 0\n",
    "        for p, r, t in zip(precision, recall, thresholds):\n",
    "            if (p+r) == 0:\n",
    "                continue\n",
    "            f_measure = 2*p*r/(p+r)\n",
    "            if f_measure > best_f_measure:\n",
    "                best_f_measure = f_measure\n",
    "                best_threshold = t\n",
    "        return np.round(best_threshold, 2)\n",
    "    \n",
    "    def get_f1_score(y_true, y_prob, threshold):\n",
    "        return f1_score([y!=0 for y in flatten(y_true)], [int(y>=threshold) for y in flatten(y_prob)])\n",
    "    \n",
    "    rs = get_rank(y_true, y_prob)\n",
    "    threshold = get_best_threshold(y_true, y_prob)\n",
    "    f_measure = get_f1_score(y_true, y_prob, threshold)\n",
    "    \n",
    "    return {\"precision_at_{}\".format(k): get_precision_at_k(rs, k), \n",
    "            \"mrr\": mean_reciprocal_rank(rs), \n",
    "            \"ndcg\": mean_ndcg_at_k(rs, 10), \n",
    "            \"threshold\": threshold, \n",
    "            \"f1_score\": f_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'delstmdense4_512_batch1024_nsrandom4_adam_lrt2t_bugfixed',\n",
       " 'model': 'DualEncoderLSTMDense4',\n",
       " 'negative_sampling': 'random',\n",
       " 'num_negative_samples': 4,\n",
       " 'epoch': 10,\n",
       " 'step': 2720000,\n",
       " 'precision_at_5': 0.3940594059405942,\n",
       " 'mrr': 0.6788464560741788,\n",
       " 'ndcg': 0.7212356813817365,\n",
       " 'threshold': 0.49,\n",
       " 'f1_score': 0.5008695652173913}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_prob = test2(infer_model, infer_sess, infer_preprocessor)\n",
    "result = {\"name\": model_config.name, \n",
    "          \"model\": model_config.model, \n",
    "          \"negative_sampling\": model_config.negative_sampling, \n",
    "          \"num_negative_samples\": model_config.num_negative_samples, \n",
    "          \"epoch\": infer_model.cur_epoch_tensor.eval(infer_sess),\n",
    "          \"step\": infer_model.global_step_tensor.eval(infer_sess)}\n",
    "result.update(evaluate_metrics(y_true, y_prob))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/angrypark/reply_matching_model/data/\"\n",
    "with open(os.path.join(base_dir, \"test_queries.txt\"), \"r\") as f:\n",
    "    test_queries = [line.strip() for line in f]\n",
    "with open(os.path.join(base_dir, \"test_replies.txt\"), \"r\") as f:\n",
    "    replies_set = [line.strip().split(\"\\t\") for line in f]\n",
    "with open(os.path.join(base_dir, \"test_labels.txt\"), \"r\") as f:\n",
    "    test_labels = [[int(y) for y in line.strip().split(\"\\t\")] for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이스크림 좋아해요?',\n",
       " '먹으면 되죠',\n",
       " '전 초코 아이스크림 좋아해요',\n",
       " '전 간식 별로 안좋아해요',\n",
       " '일하다가 당떨어졌을 때 간식같은거 먹으면 좋죠',\n",
       " '어떤 간식을 제일 좋아해요?',\n",
       " '짜장면 좋아해요?',\n",
       " '배부를 때가 제일 행복하죠',\n",
       " '저도 다이어트 해야하는데',\n",
       " '갑자기 달달한 게 땡기네요']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for query, replies, labels, probs in zip(test_queries, replies_set, y_true, y_prob):\n",
    "    sorted_replies = sorted([[r, p, l] for r, l, p in zip(replies, labels, probs)], \n",
    "                            key=lambda x: x[1], \n",
    "                            reverse=True)\n",
    "    tmp = [query]\n",
    "    for item in sorted_replies:\n",
    "        tmp.extend(item)\n",
    "    data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
