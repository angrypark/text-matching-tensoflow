{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름만 바꿔서 실행하면 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from dataset import Dataset\n",
    "from trainer import MatchingModelTrainer\n",
    "from preprocessor import Preprocessor\n",
    "from utils.dirs import create_dirs\n",
    "from utils.logger import SummaryWriter\n",
    "from utils.config import load_config, save_config\n",
    "from models.base import get_model\n",
    "from utils.utils import JamoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"new_delstm_nsrandom4echo_lr1e-3\"\n",
    "TOKENIZER = \"SentencePieceTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/media/scatter/scatterdisk/reply_matching_model/runs/{}/\".format(NAME)\n",
    "config_dir = base_dir + \"config.json\"\n",
    "best_model_dir = base_dir + \"best_loss/best_loss.ckpt\"\n",
    "# best_model_dir = base_dir + \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = load_config(config_dir)\n",
    "preprocessor = Preprocessor(model_config)\n",
    "\n",
    "infer_config = load_config(config_dir)\n",
    "setattr(infer_config, \"tokenizer\", TOKENIZER)\n",
    "setattr(infer_config, \"soynlp_scores\", \"/media/scatter/scatterdisk/tokenizer/soynlp_scores.sol.100M.txt\")\n",
    "infer_preprocessor = Preprocessor(infer_config)\n",
    "infer_preprocessor.build_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained embedding loaded. Number of OOV : 5272 / 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angrypark/angryenv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/scatter/scatterdisk/reply_matching_model/runs/new_delstm_nsrandom4echo_lr1e-3/best_loss/best_loss.ckpt\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "\n",
    "with graph.as_default():\n",
    "    Model = get_model(model_config.model)\n",
    "    data = Dataset(preprocessor, \n",
    "               model_config.train_dir, \n",
    "               model_config.val_dir, \n",
    "               model_config.min_length, \n",
    "               model_config.max_length, \n",
    "               model_config.batch_size, \n",
    "               model_config.shuffle, \n",
    "               model_config.num_epochs, \n",
    "               debug=False)\n",
    "    infer_model = Model(data, model_config)\n",
    "    infer_sess = tf.Session(config=tf_config, graph=graph)\n",
    "    infer_sess.run(tf.global_variables_initializer())\n",
    "    infer_sess.run(tf.local_variables_initializer())\n",
    "\n",
    "infer_model.load(infer_sess, model_dir=best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(preprocessor):\n",
    "    base_dir = \"/home/angrypark/reply_matching_model/data/\"\n",
    "    with open(os.path.join(base_dir, \"test_queries.txt\"), \"r\") as f:\n",
    "        test_queries = [line.strip() for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_replies.txt\"), \"r\") as f:\n",
    "        replies_set = [line.strip().split(\"\\t\") for line in f]\n",
    "    with open(os.path.join(base_dir, \"test_labels.txt\"), \"r\") as f:\n",
    "        test_labels = [[int(y) for y in line.strip().split(\"\\t\")] for line in f]\n",
    "\n",
    "    test_queries, test_queries_lengths = zip(*[preprocessor.preprocess(query)\n",
    "                                                     for query in test_queries])\n",
    "    test_replies = list()\n",
    "    test_replies_lengths = list()\n",
    "    for replies in replies_set:\n",
    "        r, l = zip(*[preprocessor.preprocess(reply) for reply in replies])\n",
    "        test_replies.append(r)\n",
    "        test_replies_lengths.append(l)\n",
    "    return test_queries, test_replies, test_queries_lengths, test_replies_lengths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sess, preprocessor):\n",
    "    test_queries, test_replies, test_queries_lengths, \\\n",
    "    test_replies_lengths, test_labels = load_test_data(preprocessor)\n",
    "\n",
    "    # flatten\n",
    "    row, col, _ = np.shape(test_replies)\n",
    "    test_queries_expanded = [[q]*col for q in test_queries]\n",
    "    test_queries_expanded = [y for x in test_queries_expanded for y in x]\n",
    "    test_queries_lengths_expanded = [[l]*col for l in test_queries_lengths]\n",
    "    test_queries_lengths_expanded = [y for x in test_queries_lengths_expanded for y in x]\n",
    "    test_replies = [y for x in test_replies for y in x]\n",
    "    test_replies_lengths = [y for x in test_replies_lengths for y in x]\n",
    "\n",
    "    feed_dict = {model.input_queries: test_queries_expanded,\n",
    "                 model.input_replies: test_replies,\n",
    "                 model.query_lengths: test_queries_lengths_expanded,\n",
    "                 model.reply_lengths: test_replies_lengths, \n",
    "                 model.embed_dropout_keep_prob: 1, \n",
    "                 model.lstm_dropout_keep_prob: 1}\n",
    "    probs = sess.run(model.positive_probs, feed_dict=feed_dict)\n",
    "    probs = np.reshape(probs, [row, col])\n",
    "    return test_labels, probs.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hackathon(model, sess, preprocessor):\n",
    "    with open(\"../paraphrase_detection/data/test_queries.txt\", \"r\") as f:\n",
    "        test_queries = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    with open(\"../paraphrase_detection/data/test_replies.txt\", \"r\") as f:\n",
    "        test_replies = [line.strip().split(\"\\t\")[1] for line in f]\n",
    "    \n",
    "    preprocessed_replies, replies_lengths = zip(*[preprocessor.preprocess(sentence) for sentence in test_replies])\n",
    "    length = len(preprocessed_replies)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        preprocessed_query, _ = preprocessor.preprocess(query)\n",
    "        feed_dict = {model.input_queries: [preprocessed_query]*length,\n",
    "                 model.input_replies: preprocessed_replies,\n",
    "                 model.queries_lengths: [len(query)]*length,\n",
    "                 model.replies_lengths: replies_lengths,\n",
    "                 model.dropout_keep_prob: 1}\n",
    "        probs = model.infer(sess, feed_dict=feed_dict)\n",
    "        probs = [(i, prob) for i, prob in enumerate(probs)]\n",
    "        probs = [(i, reply, prob) for reply, (i, prob) in zip(test_replies, probs)]\n",
    "        select = [line[1] for line in sorted(probs, key=lambda x: x[2], reverse=True)[:3]]\n",
    "        print(i, query, select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob = test(infer_model, infer_sess, infer_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_true, y_prob, k=5):\n",
    "    def get_rank(y_true, y_prob):\n",
    "        rs = list()\n",
    "        for y_t, y_p in zip(y_true, y_prob):\n",
    "            r = sorted([(t, p) for t, p in zip(y_t, y_p)], key=lambda x: x[1], reverse=True)\n",
    "            r = [t for t, p in r]\n",
    "            rs.append(r)\n",
    "        return rs\n",
    "\n",
    "    def get_precision_at_k(rs, k):\n",
    "        rs = [(np.asarray(r)[:k] != 0) for r in rs]\n",
    "        return np.mean([np.mean(r) for r in rs])\n",
    "    \n",
    "    def mean_reciprocal_rank(rs):\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    def dcg_at_k(r, k):\n",
    "        r = np.asfarray(r)[:k]\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    \n",
    "    def ndcg_at_k(r, k):\n",
    "        dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "        if not dcg_max:\n",
    "            return 0.\n",
    "        return dcg_at_k(r, k) / dcg_max\n",
    "    \n",
    "    def mean_ndcg_at_k(rs, k):\n",
    "        return np.mean([ndcg_at_k(r, k) for r in rs])\n",
    "    \n",
    "    def flatten(list_of_lists):\n",
    "        return [y for x in list_of_lists for y in x]\n",
    "    \n",
    "    def get_best_threshold(y_true, y_prob):\n",
    "        y_true_binary = [y!=0 for y in flatten(y_true)]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true_binary, flatten(y_prob))\n",
    "        best_f_measure = 0\n",
    "        best_threshold = 0\n",
    "        for p, r, t in zip(precision, recall, thresholds):\n",
    "            if (p+r) == 0:\n",
    "                continue\n",
    "            f_measure = 2*p*r/(p+r)\n",
    "            if f_measure > best_f_measure:\n",
    "                best_f_measure = f_measure\n",
    "                best_threshold = t\n",
    "        return np.round(best_threshold, 2)\n",
    "    \n",
    "    def get_f1_score(y_true, y_prob, threshold):\n",
    "        return f1_score([y!=0 for y in flatten(y_true)], [int(y>=threshold) for y in flatten(y_prob)])\n",
    "    \n",
    "    rs = get_rank(y_true, y_prob)\n",
    "    threshold = get_best_threshold(y_true, y_prob)\n",
    "    f_measure = get_f1_score(y_true, y_prob, threshold)\n",
    "    \n",
    "    return {\"precision_at_{}\".format(k): get_precision_at_k(rs, k), \n",
    "            \"mrr\": mean_reciprocal_rank(rs), \n",
    "            \"ndcg\": mean_ndcg_at_k(rs, 10), \n",
    "            \"threshold\": threshold, \n",
    "            \"f1_score\": f_measure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 4,\n",
       " 'f1_score': 0.5084745762711865,\n",
       " 'model': 'DualEncoderLSTM',\n",
       " 'mrr': 0.7052962439101053,\n",
       " 'name': 'new_delstm_nsrandom4echo_lr1e-3',\n",
       " 'ndcg': 0.7396109296005957,\n",
       " 'negative_sampling': 'random',\n",
       " 'num_negative_samples': 4,\n",
       " 'precision_at_5': 0.41089108910891087,\n",
       " 'step': 1960000,\n",
       " 'threshold': 0.49}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {\"name\": model_config.name, \n",
    "          \"model\": model_config.model, \n",
    "          \"negative_sampling\": model_config.negative_sampling, \n",
    "          \"num_negative_samples\": model_config.num_negative_samples, \n",
    "          \"epoch\": infer_model.cur_epoch_tensor.eval(infer_sess),\n",
    "          \"step\": infer_model.global_step_tensor.eval(infer_sess)}\n",
    "result.update(evaluate_metrics(y_true, y_prob))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아이스크림 좋아해요?\\t2\\t배고프면 뭐라도 먹어야죠\\t0\\t전 초코 아이스크림 좋아해요\\t2\\t전 간식 별로 안좋아해요\\t2\\t일하다가 당떨어졌을 때 간식같은거 먹으면 좋죠\\t2\\t어떤 간식을 제일 좋아해요?\\t1\\t짜장면 좋아해요?\\t0\\t배부를 때가 제일 행복하죠\\t0\\t저도 다이어트 해야하는데\\t0\\t갑자기 달달한 게 땡기네요\\t1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "for line in t.split(\"\\n\"):\n",
    "    splits = line.strip().split(\"\\t\")\n",
    "    tmp = list()\n",
    "    for i in range(10):\n",
    "        tmp.append((splits[2*i], splits[2*i+1]))\n",
    "    tmp = sorted(tmp, key=lambda x: int(x[1]), reverse=True)\n",
    "    tmp = [\"\\t\".join(item) for item in tmp]\n",
    "    result.append(\"\\t\".join(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = list()\n",
    "for line in result:\n",
    "    for item in line.strip().split(\"\\t\"):\n",
    "        if item in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "            pass\n",
    "        else:\n",
    "            if item not in replies:\n",
    "                wrong.append([line, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
