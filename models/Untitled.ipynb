{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/angrypark/text-matching-tensorflow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.models import transformer\n",
    "\n",
    "from models.base import Model\n",
    "from models.model_helper import get_embeddings, make_negative_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = transformer.transformer_base_single_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('activation_dtype', 'float32'), ('add_relative_to_values', False), ('attention_dropout', 0.1), ('attention_dropout_broadcast_dims', ''), ('attention_key_channels', 0), ('attention_value_channels', 0), ('attention_variables_3d', False), ('batch_size', 2048), ('causal_decoder_self_attention', True), ('clip_grad_norm', 0.0), ('compress_steps', 0), ('conv_first_kernel', 3), ('daisy_chain_variables', True), ('dropout', 0.2), ('eval_drop_long_sequences', False), ('eval_run_autoregressive', False), ('factored_logits', False), ('ffn_layer', 'dense_relu_dense'), ('filter_size', 2048), ('force_full_predict', False), ('grad_noise_scale', 0.0), ('heads_share_relative_embedding', False), ('hidden_size', 512), ('initializer', 'uniform_unit_scaling'), ('initializer_gain', 1.0), ('input_modalities', 'default'), ('kernel_height', 3), ('kernel_width', 1), ('label_smoothing', 0.1), ('layer_postprocess_sequence', 'da'), ('layer_prepostprocess_dropout', 0.1), ('layer_prepostprocess_dropout_broadcast_dims', ''), ('layer_preprocess_sequence', 'n'), ('learning_rate', 0.2), ('learning_rate_constant', 2.0), ('learning_rate_cosine_cycle_steps', 250000), ('learning_rate_decay_rate', 1.0), ('learning_rate_decay_scheme', 'noam'), ('learning_rate_decay_staircase', False), ('learning_rate_decay_steps', 5000), ('learning_rate_minimum', None), ('learning_rate_schedule', 'constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size'), ('learning_rate_warmup_steps', 16000), ('length_bucket_step', 1.1), ('max_input_seq_length', 0), ('max_length', 256), ('max_relative_position', 0), ('max_target_seq_length', 0), ('min_length', 0), ('min_length_bucket', 8), ('moe_hidden_sizes', '2048'), ('moe_k', 2), ('moe_loss_coef', 0.001), ('moe_num_experts', 16), ('moe_overhead_eval', 2.0), ('moe_overhead_train', 1.0), ('multiply_embedding_mode', 'sqrt_depth'), ('multiproblem_class_loss_multiplier', 0.0), ('multiproblem_label_weight', 0.5), ('multiproblem_mixing_schedule', 'constant'), ('multiproblem_reweight_label_loss', False), ('multiproblem_schedule_max_examples', 10000000.0), ('multiproblem_schedule_threshold', 0.5), ('nbr_decoder_problems', 1), ('no_data_parallelism', False), ('norm_epsilon', 1e-06), ('norm_type', 'layer'), ('num_decoder_layers', 0), ('num_encoder_layers', 0), ('num_heads', 8), ('num_hidden_layers', 6), ('optimizer', 'Adam'), ('optimizer_adafactor_beta1', 0.0), ('optimizer_adafactor_beta2', 0.999), ('optimizer_adafactor_clipping_threshold', 1.0), ('optimizer_adafactor_decay_type', 'pow'), ('optimizer_adafactor_factored', True), ('optimizer_adafactor_memory_exponent', 0.8), ('optimizer_adafactor_multiply_by_parameter_scale', True), ('optimizer_adam_beta1', 0.9), ('optimizer_adam_beta2', 0.997), ('optimizer_adam_epsilon', 1e-09), ('optimizer_momentum_momentum', 0.9), ('optimizer_momentum_nesterov', False), ('optimizer_multistep_accumulate_steps', None), ('pad_batch', False), ('parameter_attention_key_channels', 0), ('parameter_attention_value_channels', 0), ('pos', 'timing'), ('prepend_mode', 'none'), ('pretrained_model_dir', ''), ('proximity_bias', False), ('relu_dropout', 0.1), ('relu_dropout_broadcast_dims', ''), ('sampling_method', 'argmax'), ('sampling_temp', 1.0), ('scheduled_sampling_gold_mixin_prob', 0.5), ('scheduled_sampling_prob', 0.0), ('scheduled_sampling_warmup_steps', 50000), ('self_attention_type', 'dot_product'), ('shared_embedding', False), ('shared_embedding_and_softmax_weights', True), ('split_to_length', 0), ('summarize_grads', False), ('summarize_vars', False), ('symbol_dropout', 0.0), ('symbol_modality_num_shards', 16), ('symbol_modality_skip_top', False), ('target_modality', 'default'), ('tpu_enable_host_call', False), ('use_fixed_batch_size', False), ('use_pad_remover', True), ('use_target_space_embedding', True), ('video_num_input_frames', 1), ('video_num_target_frames', 1), ('vocab_divisor', 1), ('weight_decay', 0.0), ('weight_dtype', 'float32'), ('weight_noise', 0.0)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.set_hparam(\"batch_size\", self.config.batch_size)\n",
    "hparams.set_hparam(\"filter_size\", 1024)\n",
    "hparams.set_hparam(\"max_length\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.training.python.training.hparam.HParams"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n"
     ]
    }
   ],
   "source": [
    "a = transformer.TransformerEncoder(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_warmup_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HParams([('activation_dtype', 'float32'), ('add_relative_to_values', False), ('attention_dropout', 0.1), ('attention_dropout_broadcast_dims', ''), ('attention_key_channels', 0), ('attention_value_channels', 0), ('attention_variables_3d', False), ('batch_size', 4096), ('causal_decoder_self_attention', True), ('clip_grad_norm', 0.0), ('compress_steps', 0), ('conv_first_kernel', 3), ('daisy_chain_variables', True), ('dropout', 0.2), ('eval_drop_long_sequences', False), ('eval_run_autoregressive', False), ('factored_logits', False), ('ffn_layer', 'dense_relu_dense'), ('filter_size', 2048), ('force_full_predict', False), ('grad_noise_scale', 0.0), ('heads_share_relative_embedding', False), ('hidden_size', 512), ('initializer', 'uniform_unit_scaling'), ('initializer_gain', 1.0), ('input_modalities', 'default'), ('kernel_height', 3), ('kernel_width', 1), ('label_smoothing', 0.1), ('layer_postprocess_sequence', 'da'), ('layer_prepostprocess_dropout', 0.1), ('layer_prepostprocess_dropout_broadcast_dims', ''), ('layer_preprocess_sequence', 'n'), ('learning_rate', 0.2), ('learning_rate_constant', 2.0), ('learning_rate_cosine_cycle_steps', 250000), ('learning_rate_decay_rate', 1.0), ('learning_rate_decay_scheme', 'noam'), ('learning_rate_decay_staircase', False), ('learning_rate_decay_steps', 5000), ('learning_rate_minimum', None), ('learning_rate_schedule', 'constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size'), ('learning_rate_warmup_steps', 8000), ('length_bucket_step', 1.1), ('max_input_seq_length', 0), ('max_length', 256), ('max_relative_position', 0), ('max_target_seq_length', 0), ('min_length', 0), ('min_length_bucket', 8), ('moe_hidden_sizes', '2048'), ('moe_k', 2), ('moe_loss_coef', 0.001), ('moe_num_experts', 16), ('moe_overhead_eval', 2.0), ('moe_overhead_train', 1.0), ('multiply_embedding_mode', 'sqrt_depth'), ('multiproblem_class_loss_multiplier', 0.0), ('multiproblem_label_weight', 0.5), ('multiproblem_mixing_schedule', 'constant'), ('multiproblem_reweight_label_loss', False), ('multiproblem_schedule_max_examples', 10000000.0), ('multiproblem_schedule_threshold', 0.5), ('nbr_decoder_problems', 1), ('no_data_parallelism', False), ('norm_epsilon', 1e-06), ('norm_type', 'layer'), ('num_decoder_layers', 0), ('num_encoder_layers', 0), ('num_heads', 8), ('num_hidden_layers', 6), ('optimizer', 'Adam'), ('optimizer_adafactor_beta1', 0.0), ('optimizer_adafactor_beta2', 0.999), ('optimizer_adafactor_clipping_threshold', 1.0), ('optimizer_adafactor_decay_type', 'pow'), ('optimizer_adafactor_factored', True), ('optimizer_adafactor_memory_exponent', 0.8), ('optimizer_adafactor_multiply_by_parameter_scale', True), ('optimizer_adam_beta1', 0.9), ('optimizer_adam_beta2', 0.997), ('optimizer_adam_epsilon', 1e-09), ('optimizer_momentum_momentum', 0.9), ('optimizer_momentum_nesterov', False), ('optimizer_multistep_accumulate_steps', None), ('pad_batch', False), ('parameter_attention_key_channels', 0), ('parameter_attention_value_channels', 0), ('pos', 'timing'), ('prepend_mode', 'none'), ('pretrained_model_dir', ''), ('proximity_bias', False), ('relu_dropout', 0.1), ('relu_dropout_broadcast_dims', ''), ('sampling_method', 'argmax'), ('sampling_temp', 1.0), ('scheduled_sampling_gold_mixin_prob', 0.5), ('scheduled_sampling_prob', 0.0), ('scheduled_sampling_warmup_steps', 50000), ('self_attention_type', 'dot_product'), ('shared_embedding', False), ('shared_embedding_and_softmax_weights', True), ('split_to_length', 0), ('summarize_grads', False), ('summarize_vars', False), ('symbol_dropout', 0.0), ('symbol_modality_num_shards', 16), ('symbol_modality_skip_top', False), ('target_modality', 'default'), ('tpu_enable_host_call', False), ('use_fixed_batch_size', False), ('use_pad_remover', True), ('use_target_space_embedding', True), ('video_num_input_frames', 1), ('video_num_target_frames', 1), ('vocab_divisor', 1), ('weight_decay', 0.0), ('weight_dtype', 'float32'), ('weight_noise', 0.0)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size\n",
    "filter_size\n",
    "max_length\n",
    "decay_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderTransformer(Model):\n",
    "    def __init__(self, dataset, config, mode=tf.contrib.learn.ModeKeys.TRAIN):\n",
    "        super(DualEncoderTransformer, self).__init__(dataset, config)\n",
    "        if mode == \"train\":\n",
    "            self.mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "        elif (mode == \"val\") | (mode == tf.contrib.learn.ModeKeys.EVAL):\n",
    "            self.mode = tf.contrib.learn.ModeKeys.EVAL\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # build index table\n",
    "        index_table = tf.contrib.lookup.index_table_from_file(\n",
    "            vocabulary_file=self.config.vocab_list,\n",
    "            num_oov_buckets=0,\n",
    "            default_value=0)\n",
    "        \n",
    "        # get data iterator\n",
    "        self.data_iterator = self.data.get_data_iterator(index_table, mode=self.mode)\n",
    "\n",
    "        # get inputs\n",
    "        with tf.variable_scope(\"inputs\"):\n",
    "            # get next batch if there is no feeded data\n",
    "            next_batch = self.data_iterator.get_next()\n",
    "            self.input_queries = tf.placeholder_with_default(next_batch[\"input_queries\"],\n",
    "                                                             [None, self.config.max_length],\n",
    "                                                             name=\"input_queries\")\n",
    "            self.input_replies = tf.placeholder_with_default(next_batch[\"input_replies\"],\n",
    "                                                             [None, self.config.max_length],\n",
    "                                                             name=\"input_replies\")\n",
    "            self.query_lengths = tf.placeholder_with_default(tf.squeeze(next_batch[\"query_lengths\"]),\n",
    "                                                             [None],\n",
    "                                                             name=\"query_lengths\")\n",
    "            self.reply_lengths = tf.placeholder_with_default(tf.squeeze(next_batch[\"reply_lengths\"]),\n",
    "                                                             [None],\n",
    "                                                             name=\"reply_lengths\")\n",
    "\n",
    "            # get hyperparams\n",
    "            self.embed_dropout_keep_prob = tf.placeholder(tf.float64, name=\"embed_dropout_keep_prob\")\n",
    "            self.lstm_dropout_keep_prob = tf.placeholder(tf.float32, name=\"lstm_dropout_keep_prob\")\n",
    "            self.dense_dropout_keep_prob = tf.placeholder(tf.float32, name=\"dense_dropout_keep_prob\")\n",
    "            self.num_negative_samples = tf.placeholder(tf.int32, name=\"num_negative_samples\")\n",
    "\n",
    "        with tf.variable_scope(\"properties\"):\n",
    "            # length properties\n",
    "            cur_batch_length = tf.shape(self.input_queries)[0]\n",
    "\n",
    "            # learning rate and optimizer\n",
    "            learning_rate =  tf.train.exponential_decay(self.config.learning_rate,\n",
    "                                                        self.global_step_tensor,\n",
    "                                                        decay_steps=20000, decay_rate=0.96)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # embedding layer\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            embeddings = tf.Variable(get_embeddings(self.config.vocab_list,\n",
    "                                                    self.config.pretrained_embed_dir,\n",
    "                                                    self.config.vocab_size,\n",
    "                                                    self.config.embed_dim),\n",
    "                                     trainable=True,\n",
    "                                     name=\"embeddings\")\n",
    "            embeddings = tf.nn.dropout(embeddings,\n",
    "                                       keep_prob=self.embed_dropout_keep_prob,\n",
    "                                       noise_shape=[tf.shape(embeddings)[0], 1])\n",
    "            queries_embedded = tf.to_float(tf.nn.embedding_lookup(embeddings, self.input_queries, name=\"queries_embedded\"))\n",
    "            replies_embedded = tf.to_float(tf.nn.embedding_lookup(embeddings, self.input_replies, name=\"replies_embedded\"))\n",
    "        \n",
    "        # transformer layer\n",
    "        with tf.variable_scope(\"transformer\"):\n",
    "            queries_expanded = tf.expand_dims(queries_embedded, axis=2, name=\"queries_expanded\")\n",
    "            replies_expanded = tf.expand_dims(replies_embedded, axis=2, name=\"replies_expanded\")\n",
    "            \n",
    "            hparams = transformer.transformer_base()\n",
    "            query_encoder = transformer.TransformerEncoder(hparams, mode=self.mode)\n",
    "            reply_encoder = transformer.TransformerEncoder(hparams, mode=self.mode)\n",
    "            \n",
    "            queries_encoded = query_encoder({\"inputs\": queries_encoded, \n",
    "                                             \"targets\": queries_encoded})\n",
    "            replies_encoded = reply_encoder({\"inputs\": replies_encoded, \n",
    "                                             \"targets\": replies_encoded})\n",
    "            \n",
    "        # build dense layer\n",
    "        with tf.variable_scope(\"dense_layer\"):\n",
    "            M = tf.get_variable(\"M\",\n",
    "                                shape=[self.config.lstm_dim, self.config.lstm_dim],\n",
    "                                initializer=tf.initializers.truncated_normal())\n",
    "            M = tf.nn.dropout(M, self.dense_dropout_keep_prob)\n",
    "            self.queries_transformed = tf.matmul(self.queries_encoded, tf.cast(M, tf.float64))\n",
    "\n",
    "        with tf.variable_scope(\"sampling\"):\n",
    "            self.distances = tf.matmul(self.queries_transformed, self.replies_encoded, transpose_b=True)\n",
    "            positive_mask = tf.reshape(tf.eye(cur_batch_length), [-1])\n",
    "            negative_mask = tf.reshape(make_negative_mask(self.distances,\n",
    "                                                          method=self.config.negative_sampling,\n",
    "                                                          num_negative_samples=self.num_negative_samples), [-1])\n",
    "            \n",
    "        with tf.variable_scope(\"prediction\"):\n",
    "            distances_flattened = tf.reshape(self.distances, [-1])\n",
    "            self.positive_logits = tf.gather(distances_flattened, tf.where(positive_mask), 1)\n",
    "            self.negative_logits = tf.gather(distances_flattened, tf.where(negative_mask), 1)\n",
    "\n",
    "            self.logits = tf.concat([self.positive_logits,\n",
    "                                     self.negative_logits], axis=0)\n",
    "            self.labels = tf.concat([tf.ones_like(self.positive_logits),\n",
    "                                     tf.zeros_like(self.negative_logits)], axis=0)\n",
    "            \n",
    "            self.positive_probs = tf.sigmoid(self.positive_logits)\n",
    "\n",
    "            self.probs = tf.sigmoid(self.logits)\n",
    "            self.predictions = tf.cast(self.probs > 0.5, dtype=tf.int32)\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.logits))\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        with tf.variable_scope(\"score\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.to_int32(self.labels))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "angryenv",
   "language": "python",
   "name": "angryenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
